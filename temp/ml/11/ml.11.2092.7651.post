By watching <ref name="video" type="sai">the video</ref> I understood that NN cost function is not convex so the gradient descent will only find a local optimum, and also the concept of symmetry breaking by random initialization, but I was wondering why need symmetry breaking? Although initializing all thetas to 0 will lead to redundant features in all the hidden layers, but as a cost function, no matter what initial value we choose, it should converge into a local minimum anyway, why all 0s initialization is an exception? 

I actually tried to run <ref name="file" type="sai">ex4</ref> without random initialization, the accuracy is very low and the cost is very high, but as the cost function is hard to visualize, I'm not sure what exactly caused this. Could some one explain what exactly happens in the gradient descent with all-0 initialization? Thank you!
