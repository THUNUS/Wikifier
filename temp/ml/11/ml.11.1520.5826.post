You could do this, you would need the four recognition neurons in a hidden layer and then apply two boolean functions to get the encoded outputs. If you put the right output function in place during training it should do this anyway. In fact its a good idea to try once your back propagation training is working, but you might need to add an extra layer or two. It would be interesting to see if the last two layers function in the way expected.
I think the general idea though is to represent continuous variables as a row of outputs with a Gaussian activation over them. This goes back to biology where  there is multiple redundancy, but there is a higher level purpose to it I think, its speculative but once you start organizing the hidden layers (as you need to to reduce connections between layers) you find it is best not to work with orthonormal basis sets or encoded data, but actually have redundant neurons in between the minimal set, so as to help the cortical self organization process. So from this perspective encoding the outputs in this way would not allow cortical self organization.
I hope it answers the question. Its the general objective of preserving the essential features from biology where possible, once they are fully understood then you can perhaps go beyond them.
