Although I've finished all but week 10, I just finished looking up cross-validation in the Witten Data Minining Book and I agree with you.  The way it is explained in the videos I don't see any difference in the cross-validation set and the test set.  The k-fold as described by Witten et. al.  does a very good job of explaining this issue.  I feel that Prof. Ng did not want to detract from the details of the learning algorithms by discussing the issue in depth.  I've been bothered by the video, however, since I first saw it and that is what prompted me review the Data Mining book based on Weka.  In my opionion the 2 fold is better.  It is more of a test.  What are the consequences of a bad CV run over those of the CV + test set run.  Thank you for bringing this up.  I've been hesitant to do so.
