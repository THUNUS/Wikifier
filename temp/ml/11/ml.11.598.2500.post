You see what I'm doing isn't "quibbling" over terms.  It just so happens that you have a non-Bayesian perspective of learning (the posterior probability), and I happen to know for a fact (first person in history to prove it) that inside every non-Bayesian there's a Bayesian screaming to get out.

But don't take my word for it.  Here's the office of naval research (ONR) offering a $3.85 million contract to whoever can figure out implicit learning.  I submitted my solution on the 16 April.

[OFFICE OF NAVAL RESEARCH Contract for Implicit Learning - The Sixth Sense][1]

So trust me, the Bayesian method of [machine] learning wipes the floor with prediction.  Here's an example.  If you were the smartest stock analyst in the world, while you're focused on prediction, I'm focused on an algorithm that clones your decisions.  Why is that valuable.  Well if you're the smartest analyst in the world, then we can't do any better than you, so cloning you wouldn't have any effect on the likelihood of you predicting the future.  But if you assert that somehow you can do better than not knowing the future, then cloning you (your decisions) into a machine would have some sort of effect.  Cloning would be better than prediction because more of you would be more likely to arrive at the solution.  But this isn't possible if you assume uncertainty.  So what would you do with a whole bunch of clones?  You probably wouldn't have all of them working on the same problem.  You'd have them do other stuff for you.  Cloning erases the facade of time.  Time is non-existent.  Thus neither is the posterior.  I need only to prove the prior to prove that, and I can prove the prior probability.  Self-replicating (Von Neumann) creates quantum computing.  Again, this isn't semantics; I'd be happy to put my AI (20kb) up against anyone else's. 


  [1]: https://www.fbo.gov/index?s=opportunity&mode=form&id=723de7fc46213a209552d9131dcf2132&tab=core