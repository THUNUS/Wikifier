http://en.wikipedia.org/wiki/Cross-validation_(statistics)  describes 2 Fold cross validation, K Fold Validation and leave one out cross validation. In that the idea is basically to switch the test and training sets and see the difference in errors after flipping the training and test sets. However, Prof Andrew speaks about a fixed cross validation set (where is the cross validation there?) that is used to assess the validation error and pick the best model. The training set is also kept fixed. My question is it better to use 2 Fold cross validation instead of dividing the data into 3 sets (training set, "cross validation set" and test set). Am I missing something?