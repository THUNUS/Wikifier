I think it's important to note that this discussion really depends on what learning algorithm your using. There are learning algorithms who's underpinnings do not really assume a differentiable cost function: genetic algorithms, simulated annealing, ID3, and Kohonen SOM networks all come to mind. Others are absolutely bound by their mathematical underpinnings to need, or inherently have, a differentiable error function. Most of the hill climbers and steepest descent algorithms are good examples, I believe.