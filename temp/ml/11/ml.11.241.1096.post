Here's a beautiful derivation prof. Yaser Abu-Mostafa does in [his lecture][2] (I just adopted the notation).

---

So, the task is to draw a line through a set of points and minimize the "vertical discrepancies" between the _training set_ points $$(x,y)$$ and their corresponding _prediction line_ points$$(x, \hat{y})$$. So we define a cost function which is just an averaged sum of those discrepancies: 

$$J(\theta)=(1/2m)*\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^{2}$$ 

---

We can rewrite this as $$J(\theta)=(1/2m)*\sum_{i=1}^{m} (\theta^{T}x^{(i)} - y^{(i)})^{2}$$ 

and then as $$J(\theta)=(1/2m)*||X\theta- y||^2$$, with $$X$$ being a matrix where i-th row contains all $$x^{(i)}$$ values (same as in Andrew Ng's videos, I guess).

Finally, the same in a matrix form:

$$J(\theta)=(1/2m)*(X\theta - y)^T(X\theta - y)$$. 

---

We need to minimize the above function and, as in any minimization problem, we take a derivative (with respect to $$\theta$$) and equate it to zero. Note, as we're working with matrices it's not "just 0" but a vector of zeros: 

$$\triangledown J=1(/2m) \cdot X^{T} \cdot (X\theta - y) = 0$$ 

$$X^{T} \cdot (X\theta - y) = 0$$ 

$$X^{T}X\theta = X^{T}y$$ 

$$\theta = (X^{T}X)^{-1}X^{T}y$$ 


  [1]: https://class.coursera.org/ml/forum/thread?thread_id=71
  [2]: http://www.youtube.com/watch?feature=player_detailpage&v=FIbVs5GbBlQ#t=2048s