It is due to the variance using the square deviation from the mean. In turn, the variance ends up in the Gaussian function (which describes normal distributions). Furthermore, using Bayesian reasoning, it can be shown that the hypothesis that has the highest probability of correctly predicting unseen instances, given the training data, can be found by minimizing the Gaussian function (the derivation can be found in T. Mitchell's Machine Learning, which I highly recommend). Minimizing the Gaussian results in the sum of least squares method.

In other words, to answer the question, you should ask why the Gauss function is defined using squares instead of absolute values. The answer to this question follows from the Central Limit Theorem, which states that a large number of independent trials approximates the normal distribution, which has a very nice proof.