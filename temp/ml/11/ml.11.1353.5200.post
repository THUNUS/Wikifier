Hi: As always, Andrew does a nice job but I was trying to reconcile his backpropagation formulae
with those in various books/papers. I kind of am able to but I just had one question:

Since he uses the cost function involving the log(h(x), is he making the implicit assumption that
the OUTPUT neurons of the network have an activation function that is logistic. I get confused
because he defines the error at the output neuron as the difference between the target and y_i
so I'm thinking that y_i might be the just the output directly with the identity activation function
but I'm not sure.

Also, if you know of a derivation in the literature that makes the same assumptions about the output
neuron as andrew, ( I think most of them assume the identity but they are not so clear on this ),
I'd love to read one. Most of them use a generalized delta rule without ever referring to
the partial derivative of the cost function J(theta) but I guess Andrew's is equivalent to theirs.

For anyone reading this, if you're looking for a clear derivation of backpropagation, the best explanation I have found is Chapter 5 of D.Kreisel : "The perceptron, backpropagation and it's variants".  It's on the internet at this link below: I'm not quite sure if it's EXACTLY equivalent to Andrew's formulation but it's very close and very clear. ( Andrew is clear also but he obviously has to skip some of the gory details ).

http://www.scribd.com/doc/44397618/Neural-Networks

















