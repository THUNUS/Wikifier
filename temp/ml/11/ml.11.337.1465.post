I have a question on one step gradient descent algorithm. If we initialize a point that is already a local minimum, then the one step gradient descent won't update the value of the variable. However, in one of the examples in the Gradient descent intuition lecture, the function has a global minimum as well. How can we adjust our algorithm so it can also reach the global minimum as well? If we keep running the algorithm with the same initial value at the local minimum, then there will be no updates, is that correct? Thank you very much for your time. -Farhad