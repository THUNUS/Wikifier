That's the entire point of gradient descent. The partial derivative of the cost function tells the algorithm which way to go. You always (for a sufficiently small learning rate) go towards the global mininum, regardless of the initial choices for theta. You're right about inefficiently choosing theta at first, but no matter what you'll eventually reach the optimal value. In one of the lectures, Professor Ng discusses why the partial derivative points theta in the right direction. I wouldn't be suprised if we learned how to efficiently choose an initial theta later in the course.