I'll do my best to explain this. I believe that it's usually the case where you won't find an analytical solution to problems; the real interesting case is why you get a simple answer for least-squares regression.

Here's the basic problem with linear least-squares regression. Say you have $$m$$ features and $$n$$ data points, with $$n \ge m$$. Since you have $$n$$ data points, your data can be represented by a vector in $$\mathbb{R}^n$$. Similarly, your predictions can also be represented by a vector in $$\mathbb{R}^n$$. But because your predictions are $$X \theta$$ and since $$\theta$$ is a vector in $$\mathbb{R}^m$$, your predictions must lie in an $$m$$-dimensional subspace of $$\mathbb{R}^n$$. Basically, all your possible predictions form an $$n$$-dimensional plane in $$m$$-dimensional space.

With logistic regression, the parameters don't linearly affect the output, so you wouldn't get a plane. And I'm not sure that the cost function for logistic regression can be the norm of a vector space, either.

The real cool thing is to imagine the cost function in this view. Note that $$J ( \theta ) = || y - X \theta ||^2$$. It's just the (squared) distance between the prediction and the actual dataset in $$n$$-dimensional space. So you have to pick a point from the $$m$$-dimensional plane that minimizes that distance. From intuition, we can see that that best point that we pick on the plane will make it so that the vector $$y - X \theta$$, the vector from the actual dataset to the prediction, is orthogonal/perpendicular to the plane. Thus the prediction $$X \theta$$ is just the orthogonal projection of $$y$$ onto the plane, and from linearly algebra, we have an analytic way to compute that.