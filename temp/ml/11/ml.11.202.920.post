At <ref name="video" type="sci">about 09:08 of the video 2-6</ref>, it stats that " Gradient descent can converge to a local minimum, even with the learning rate $$\alpha$$ fixed".  I understand that the gradient becomes small when approach to the minimum. But what if $$\alpha$$ is large?  Previously, Prof. Ng say that it may not converge or even diverge.   Probably it's assumed that we had chosen a "good" $$\alpha$$.  
