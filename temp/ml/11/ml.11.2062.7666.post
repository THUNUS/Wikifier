In SVM, if you reduce C ~ 1/lambda, you allow more cost to exist in the first two (misclassification) terms since that cost is multiplied by smaller number. That would allow some X's and O's to violate their respective boundaries, resulting in a smoother boundary curve. This is of course SVM with "kernels" since the SVM boundary without kernels is always planar. In some sense, violating boundaries like this would be associated with a larger than normal "gap" which in turn is associated with a smaller |theta|. Conversely, a large C magnifies those misclassification terms forcing them in effect to all be zero which in turn disallows training samples to be in the gap which in turn forces the x space boundary to do acrobatic moves (high variance). I have attempted to write some notes on SVM and other Ng stuff in a document at http://rimrockinteractive.net/plucht/. They do show for example why theta is the normal to the boundary plane. 