The dependent features wouldn't be over-represented, they just wouldn't add anything to the performance (as in accuracy) of the model. If all features were linear combinations of each other, you would just end up with the same high-bias result as if you trainined on a single feature. So, to answer your question - no, dropping a dependent feature will not increase accuracy. However, replacing it with a new non-dependent feature would be a better strategy.